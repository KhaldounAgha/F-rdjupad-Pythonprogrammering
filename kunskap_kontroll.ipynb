{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd     # Pandas library for data manipulation and analysis.\n",
    "import logging          # Logging module for tracking and logging execution events.\n",
    "import datetime as dt   # Datetime module for manipulation of date and time.\n",
    "import os               # OS module for interacting with the operating system.\n",
    "\n",
    "class DataCleaner:\n",
    "    \n",
    "    def __init__(self, dir: str):\n",
    "        self.directory = dir\n",
    "        self.log_path = None\n",
    "        self.data_path = None\n",
    "        self.log_filename: str = \"program_logging.log\"\n",
    "        self.data_filename: str = \"Students_Performance.csv\"\n",
    "        self.column_list: list[str] = ['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course', 'math score', 'reading score', 'writing score', 'date']\n",
    "        self.encoding_list: list[str] = [\"utf-8\", \"utf-16\", \"ISO-8859-1\", \"cp1252\"]\n",
    "        self.nan_list: list[str] = [\"?\", \"NA\", \"n/a\", \"na\", \"Null\", \"NaN\", \"#####\"]\n",
    "        self.exclude_columns: list[str] = None\n",
    "        self.df: pd.DataFrame = None\n",
    "\n",
    "    def directory_path(self) -> str:\n",
    "        \"\"\"\n",
    "        Ensures that a valid directory path exists for the log file and data file. \n",
    "        Returns:\n",
    "            str: The valid directory path.\n",
    "        \"\"\"\n",
    "        directory: str = self.directory\n",
    "        \n",
    "        if not os.path.isdir(directory):\n",
    "            print(f\"Warning: [The directory '{directory}' does not exist]\")\n",
    "            logging.warning(f\"The directory [{directory}] does not exist. Prompting user for a new path.\")\n",
    "            \n",
    "            while True:\n",
    "                directory = input(\"Please enter the correct directory path: \")\n",
    "                \n",
    "                if os.path.isdir(directory):\n",
    "                    logging.info(f\"Valid directory path provided by the user: [{directory}]\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Warning: [The entered directory '{directory}' does not exist]\")\n",
    "                    logging.warning(f\"User provided an invalid directory path: [{directory}]. Prompting for a new path.\")\n",
    "        \n",
    "        self.directory = directory\n",
    "        return self.directory\n",
    "        \n",
    "    def logging_path(self) -> str:\n",
    "        \"\"\"\n",
    "        Retrieves or creates the log file and returns the path.\n",
    "        Returns:\n",
    "            str: The valid path of the log file. \n",
    "        \"\"\"\n",
    "        directory: str = self.directory_path()\n",
    "        log_filename: str = self.log_filename\n",
    "        \n",
    "        log_path: str = os.path.join(directory, log_filename)\n",
    "        \n",
    "        if not os.path.isfile(log_path):\n",
    "            logging.info(f\"[{log_filename}] does not exist. A new file will be created.\")\n",
    "            try: \n",
    "                open(log_path, \"a\").close()\n",
    "                logging.info(f\"Log file [{log_filename}] has been created at [{directory}]\")\n",
    "                self.log_path = log_path\n",
    "                return self.log_path\n",
    "                \n",
    "            except OSError as e:\n",
    "                print(f\"Operating System Error creating [{log_filename}] file: {e}\")\n",
    "                logging.warning(f\"Operating System Error creating [{log_filename}] file: {e}\")\n",
    "                return \"\"\n",
    "            \n",
    "        else:\n",
    "            logging.info(f\"Log file [{log_filename}] already exists\")\n",
    "            self.log_path = log_path\n",
    "            return self.log_path\n",
    "        \n",
    "    def configure_logging(self):\n",
    "        \"\"\"\n",
    "        Configures the logging with the log file path.\n",
    "        \"\"\"\n",
    "        log_path: str = self.logging_path()\n",
    "        \n",
    "        logging.basicConfig(\n",
    "            filename=log_path, \n",
    "            filemode=\"a\", \n",
    "            force=True, \n",
    "            level=logging.INFO, \n",
    "            format=\"[%(asctime)s][%(levelname)s]: [%(message)s]\"\n",
    "        )\n",
    "       \n",
    "    def dataset_path(self) -> str:\n",
    "        \"\"\"\n",
    "        Retrieves the path for the data source file.\n",
    "        Returns:\n",
    "            str: The valid path of the data source file.    \n",
    "        \"\"\"\n",
    "        directory: str = self.directory_path()\n",
    "        data_filename: str = self.data_filename\n",
    "        \n",
    "        data_path: str = os.path.join(directory, data_filename)\n",
    "        \n",
    "        if not os.path.exists(data_path):\n",
    "            print(\"Warning: [The data source does not exist]\")\n",
    "            logging.warning(f\"Data source [{data_filename}] does not exist at [{directory}]. Prompting user for a new path.\")\n",
    "                \n",
    "            while True:\n",
    "                data_path: str = input(\"Please enter the correct path for the data file: \")\n",
    "                \n",
    "                if os.path.exists(data_path):\n",
    "                    logging.info(f\"New data source file has been identified by the user at: [{data_path}]\")\n",
    "                    break\n",
    "                \n",
    "                else:\n",
    "                    print(\"Warning: [The entered data source path does not exist]\")\n",
    "                    logging.warning(f\"User provided an invalid data path: [{data_path}]. Please re-enter.\")\n",
    "\n",
    "\n",
    "        logging.info(f\"Data source [{data_filename}] exists at [{directory}]\")\n",
    "        self.data_path = data_path\n",
    "        return self.data_path\n",
    "\n",
    "    def read_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Reads data from a CSV file using various encodings and replaces certain specified strings \n",
    "        as missing values (NaN).\n",
    "        Returns: \n",
    "            pd.DataFrame: The data from the CSV file.\n",
    "        \"\"\"\n",
    "        data_path: str = self.dataset_path()\n",
    "        encoding_list: list[str] = self.encoding_list\n",
    "        nan_list: list[str] = self.nan_list\n",
    "        \n",
    "        for encoding in encoding_list:\n",
    "            try:\n",
    "                df: pd.DataFrame = pd.read_csv(\n",
    "                    data_path, \n",
    "                    encoding=encoding, \n",
    "                    keep_default_na=True, \n",
    "                    na_values=nan_list, \n",
    "                    header=0\n",
    "                )\n",
    "                \n",
    "                logging.info(f\"Successfully read file with encoding: {encoding.upper()}\")\n",
    "                self.df = df\n",
    "                return self.df\n",
    "            \n",
    "            except UnicodeError as e:\n",
    "                logging.warning(f'UnicodeError with encoding {encoding.upper()}: {str(e)}')\n",
    "            \n",
    "            except FileNotFoundError:\n",
    "                logging.error(f\"File not found: {data_path}\")\n",
    "                raise FileNotFoundError(f\"Could not find file at: {data_path}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error reading file with encoding {encoding.upper()}: {str(e)}\")\n",
    "                print(f\"Encoding attempt using {encoding.upper()} failed for the file.\")\n",
    "        \n",
    "        raise ValueError(f\"All encoding attempts failed for the data file at: [{data_path}]\")\n",
    "       \n",
    "    def rename_columns(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Renames the header of the columns in the dataframe.\n",
    "        Returns: \n",
    "            pd.DataFrame: DataFrame with renamed columns or original DataFrame if lengths do not match.\n",
    "        \"\"\"\n",
    "        df = self.read_data()\n",
    "        \n",
    "        old_names = df.columns\n",
    "        new_names = self.column_list\n",
    "        \n",
    "        if len(old_names) != len(new_names):\n",
    "            logging.warning(f\"Number of old columns ({len(old_names)}) does not match number of new names ({len(new_names)}). No changes have been made to the column headers.\")\n",
    "            return df \n",
    "        \n",
    "        df = df.rename(columns=dict(zip(old_names, new_names)))\n",
    "        \n",
    "        logging.info(f\"Renamed dataframe columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        self.df = df\n",
    "        return self.df    \n",
    "                \n",
    "    def dataset_info(self) -> None:\n",
    "        \"\"\"\n",
    "        Log information about the dataset, including duplicates and missing values.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Log DataFrame info\n",
    "        logging.info(f\"Dataset Shape: Rows [{df.shape[0]}] : Columns [{df.shape[1]}]\")\n",
    "        logging.info(f\"Column Names: Rows {df.columns.tolist()}\")\n",
    "        logging.info(f\"Column Data Types: {[f\"{col}: {dtype}\" for (col, dtype) in df.dtypes.items()]}\")\n",
    "        logging.info(f\"Null Value Counts: {df.isna().sum().to_dict()}\") \n",
    "        \n",
    "        # Calculate missing values and duplicates efficiently\n",
    "        missing_count = df.isna().sum()\n",
    "        duplicated_count = df.duplicated().sum()\n",
    "        \n",
    "        # Log missing values for non-zero counts\n",
    "        missing_info = missing_count[missing_count > 0].to_dict()\n",
    "        logging.info(f\"Missing values: {missing_info}\")\n",
    "        \n",
    "        # Log duplicate information\n",
    "        logging.info(f\"Duplicated rows: the dataset has {duplicated_count} duplicated rows.\")\n",
    "\n",
    "    def drop_duplicated(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Remove duplicate rows from the dataframe.\n",
    "        Returns: \n",
    "            pd.DataFrame: DataFrame with duplicates removed\n",
    "        \"\"\"\n",
    "        df: pd.DataFrame = self.df\n",
    "        \n",
    "        # Store the number of rows before and after dropping duplicates\n",
    "        rows_before = len(df)\n",
    "        \n",
    "        df.drop_duplicates(keep=\"first\", inplace=True)\n",
    "        \n",
    "        rows_after = len(df)\n",
    "        rows_dropped = rows_before - rows_after\n",
    "\n",
    "        # Log the number of duplicates dropped and the new total number of rows\n",
    "        logging.info(f\"DUPLICATED rows: dropped [{rows_dropped}] out of [{rows_before}] rows. The new dataset includes: [{rows_after}] rows.\")\n",
    "        \n",
    "        self.df = df\n",
    "        return self.df\n",
    "\n",
    "    def drop_missing(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Remove rows with missing values in specified columns.\n",
    "        Returns: \n",
    "        pd.DataFrame: DataFrame with rows containing missing values removed\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        exclude_columns = self.exclude_columns\n",
    "\n",
    "        rows_before = len(df)\n",
    "        \n",
    "        if exclude_columns is None:\n",
    "            exclude_columns = []\n",
    "        \n",
    "        subset = [col for col in df.columns if col not in exclude_columns]\n",
    "        df.dropna(axis=0, how=\"any\", subset=subset, inplace=True)\n",
    "        \n",
    "        rows_after = len(df)\n",
    "        rows_dropped = rows_before - rows_after\n",
    "\n",
    "        logging.info(f\"MISSING values: dropped {rows_dropped} rows out of {rows_before} rows. New dataset has {rows_after} rows.\")\n",
    "\n",
    "        self.df = df\n",
    "        return self.df\n",
    "\n",
    "    def strip_columns(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Strips leading and trailing whitespace in columns with string datatype.\n",
    "        Returns: \n",
    "            pd.DataFrame: DataFrame with cleaned string columns\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        info: dict[str, int] = {}\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == object or df[col].dtype == 'string':\n",
    "                original_values = df[col].values\n",
    "                df[col] = df[col].str.strip()\n",
    "                stripped_count: int = int(sum(original_values != df[col].values))\n",
    "                info[col] = int(stripped_count)\n",
    "\n",
    "        logging.info(f\"Number of stripped leading and trailing whitespace: {info}\")    \n",
    "\n",
    "        self.df = df\n",
    "        return self.df\n",
    "    \n",
    "    def clean_values(self) -> pd.DataFrame:\n",
    "        \"\"\"_summary_\n",
    "        Cleans values in DataFrame columns\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with cleaned values\n",
    "        \"\"\"\n",
    "\n",
    "        df = self.df\n",
    "\n",
    "        correction_dict: dict = {\n",
    "            \"gender\": {\n",
    "                \"female\": [\"F\", \"f\", \"Fe\", \"fe\"],\n",
    "                \"male\": [\"M\", \"m\", \"Ma\", \"ma\"]\n",
    "            },\n",
    "            \"race/ethnicity\": {\n",
    "                \"group A\": [\"A\", \"a\", \"groupA\", \"groupa\", \"group A\", \"group a\"],\n",
    "                \"group B\": [\"B\", \"b\", \"groupB\", \"groupb\", \"group B\", \"group b\"],\n",
    "                \"group C\": [\"C\", \"c\", \"groupC\", \"groupc\", \"group C\", \"group c\"],\n",
    "                \"group D\": [\"D\", \"d\", \"groupD\", \"groupd\", \"group D\", \"group d\"],\n",
    "                \"group E\": [\"E\", \"e\", \"groupE\", \"groupe\", \"group E\", \"group e\"]\n",
    "            },\n",
    "            \"lunch\": {\n",
    "                \"free/reduced\": [\"free/???\", \"free/\\\\reduced\"]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        expected_dict: dict = {\n",
    "            \"gender\": [\"female\", \"male\"],\n",
    "            \"race/ethnicity\": [\"group A\", \"group B\", \"group C\", \"group D\", \"group E\"],\n",
    "            \"parental level of education\": [\"some high school\", \"high school\", \"some college\", \"associate's degree\", \"bachelor's degree\", \"master's degree\"],\n",
    "            \"test preparation course\": [\"none\", \"completed\"],\n",
    "            \"lunch\": [\"free/reduced\", \"standard\"]\n",
    "        }\n",
    "\n",
    "        for col in correction_dict:\n",
    "            if col not in df.columns:\n",
    "                logging.warning(f\"Column [{col}] not found in the DataFrame.\")\n",
    "                continue\n",
    "            logging.info(f\"Column [{col}] original values: {df[col].value_counts().to_dict()}\")\n",
    "            _dict: dict = {val: key\n",
    "                            for key in correction_dict[col] \n",
    "                            for val in correction_dict[col][key]\n",
    "                            }\n",
    "            df[col] = df[col].replace(_dict)\n",
    "            logging.info(f\"Column [{col}] cleaned values:{df[col].value_counts().to_dict()}\")\n",
    "        \n",
    "        for col in correction_dict:\n",
    "            if col in expected_dict:\n",
    "                expected_values: list[str] = [key for key in expected_dict[col]]\n",
    "                unique_values: list[str] = list(df[col].unique())\n",
    "                unexpected_values : list[str] = [val for val in unique_values if val not in expected_values]\n",
    "                if unexpected_values:\n",
    "                    logging.warning(f\"Unexpected values has been found in [{col}] column after cleaning: {unexpected_values}\")\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        self.df = df\n",
    "        return self.df\n",
    "\n",
    "    def clean_date(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Cleans date column replacing multiple consecutive slashes with a single slash.\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with cleaned date column.\n",
    "        \"\"\"\n",
    "        \n",
    "        df = self.df\n",
    "        \n",
    "        initial_count = df[\"date\"].str.count(\"//\").sum()\n",
    "        \n",
    "        df[\"date\"] = df[\"date\"].str.replace(r\"/+\", \"/\", regex=True)\n",
    "        \n",
    "        if df[\"date\"].str.contains(\"//\").any():\n",
    "            logging.warning(\"Not all consecutive slashes were replaced in the 'date' column\")\n",
    "            \n",
    "        remaining_count = df[\"date\"].str.count(\"//\").sum()\n",
    "        \n",
    "        replaced_count = initial_count - remaining_count\n",
    "        \n",
    "        logging.info(f\"[{replaced_count}] consecutive slashes were replaced in the 'date' column\")     \n",
    "        \n",
    "        if remaining_count > 0:\n",
    "            logging.warning(f\"[{remaining_count}] double slashes remain in the 'date' column\")\n",
    "\n",
    "        self.df = df\n",
    "        return self.df\n",
    "\n",
    "    \n",
    "    def process_date_column(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process the 'date' column in a DataFrame to standardize date format.\n",
    "        Returns: pd.DataFrame: DataFrame with processed 'date' column.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        def parse_date(date_string: str) -> dt.datetime:\n",
    "            \"\"\"\n",
    "            Parse a date string using multiple formats.\n",
    "            Args: date_string (str): The date string to parse.\n",
    "            Returns: dt.datetime: Parsed datetime object if successful, date_string (str) if not.\n",
    "            \"\"\"\n",
    "            date_formats = [\n",
    "                \"%d.%m.%Y\",  # for \"15.03.2020\"\n",
    "                \"%d%m%Y\",    # for \"15032020\"\n",
    "                \"%d/%m/%Y\",  # for \"15/03/2020\"\n",
    "                \"%d-%m-%Y\",  # for \"15-03-2020\"\n",
    "                \"%m/%d/%Y\",  # for \"03/15/2020\"\n",
    "                \"%Y/%m/%d\"   # for \"2020/03/15\"\n",
    "            ]\n",
    "\n",
    "            for fmt in date_formats:\n",
    "                try:\n",
    "                    return dt.datetime.strptime(date_string, fmt)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            \n",
    "            logging.warning(f\"Unable to parse date: {date_string}\")\n",
    "\n",
    "            return date_string\n",
    "        \n",
    "        df[\"date\"] = df[\"date\"].astype(str)\n",
    "        df[\"date\"] = df[\"date\"].apply(parse_date)\n",
    "        df[\"date\"] = df[\"date\"].apply(lambda val: dt.datetime.strftime(val, \"%Y-%m-%d\") if isinstance(val, dt.datetime) else val)\n",
    "        \n",
    "        self.df = df\n",
    "        return self.df\n",
    "\n",
    "# Execution\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        cleaner = DataCleaner(dir=r\"C:\\Users\\aghaa\\OneDrive\\Desktop\\Kunskap_2\")\n",
    "        cleaner.directory_path()\n",
    "        cleaner.logging_path()\n",
    "        cleaner.configure_logging()\n",
    "        cleaner.dataset_path()\n",
    "        cleaner.read_data()\n",
    "        cleaner.rename_columns()\n",
    "        cleaner.dataset_info()\n",
    "        \n",
    "        cleaner.drop_duplicated()\n",
    "        cleaner.drop_missing()\n",
    "        cleaner.strip_columns()\n",
    "        cleaner.clean_values()\n",
    "        cleaner.clean_date()\n",
    "        cleaner.process_date_column()\n",
    "        \n",
    "        output_dir: str = r\"C:\\Users\\aghaa\\OneDrive\\Desktop\\Kunskap_2\"\n",
    "        output_file: str = \"cleaned_students_performance.csv\"\n",
    "        output_path: str = os.path.join(output_dir, output_file)\n",
    "        try: \n",
    "            cleaner.df.to_csv(path_or_buf= output_path, mode=\"w\", index=False)\n",
    "            logging.info(f\"Cleaned data file saved at [{output_path}]\")\n",
    "        except Exception as sav:\n",
    "            logging.error(f\"File saving ERROR occered [{sav}]\")\n",
    "    \n",
    "    except Exception as per:\n",
    "        logging.error(f\"File processing ERROR occered: [{per}]\")\n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
